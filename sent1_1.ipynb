{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GUr2D5e4-j0",
        "outputId": "e8dcd854-6832-44dd-9efb-00c108f7e307"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - accuracy: 0.3542 - loss: 0.6971 - val_accuracy: 0.5000 - val_loss: 0.6850\n",
            "Epoch 2/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8542 - loss: 0.6573 - val_accuracy: 0.5000 - val_loss: 0.6780\n",
            "Epoch 3/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7292 - loss: 0.6407 - val_accuracy: 0.5000 - val_loss: 0.6690\n",
            "Epoch 4/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.6008 - val_accuracy: 0.5000 - val_loss: 0.6582\n",
            "Epoch 5/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.5555 - val_accuracy: 0.5000 - val_loss: 0.6473\n",
            "Epoch 6/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.5372 - val_accuracy: 0.5000 - val_loss: 0.6368\n",
            "Epoch 7/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.5132 - val_accuracy: 0.5000 - val_loss: 0.6248\n",
            "Epoch 8/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.4781 - val_accuracy: 1.0000 - val_loss: 0.6126\n",
            "Epoch 9/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.4319 - val_accuracy: 1.0000 - val_loss: 0.5965\n",
            "Epoch 10/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.4232 - val_accuracy: 1.0000 - val_loss: 0.5839\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 1.0000 - loss: 0.4176\n",
            "Model accuracy: 100.00%\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "Comment: 'I absolutely love this product!' -> Sentiment: Positive\n",
            "Comment: 'Do not buy this, it’s horrible.' -> Sentiment: Negative\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, Flatten\n",
        "\n",
        "# Sample data: Comments and sentiment labels (1 = positive, 0 = negative)\n",
        "data = {\n",
        "    'comment': [\n",
        "        'I love this product!',\n",
        "        'This is the worst purchase I have made.',\n",
        "        'Amazing quality, will buy again!',\n",
        "        'Terrible, waste of money.',\n",
        "        'Very satisfied with the service.',\n",
        "        'I regret buying this, it’s awful.',\n",
        "        'Excellent experience, highly recommend!',\n",
        "        'Do not buy this, it’s horrible.'\n",
        "    ],\n",
        "    'sentiment': [1, 0, 1, 0, 1, 0, 1, 0]  # 1 = positive, 0 = negative\n",
        "}\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Preprocess the text data\n",
        "tokenizer = Tokenizer(num_words=1000)  # Limit to 1000 most common words\n",
        "tokenizer.fit_on_texts(df['comment'])\n",
        "sequences = tokenizer.texts_to_sequences(df['comment'])\n",
        "max_length = max([len(seq) for seq in sequences])  # Find max sequence length\n",
        "X = pad_sequences(sequences, maxlen=max_length)\n",
        "y = df['sentiment'].values\n",
        "\n",
        "# Build the neural network model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=1000, output_dim=64, input_length=max_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=10, batch_size=2, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X, y)\n",
        "print(f\"Model accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Making predictions\n",
        "comments = [\n",
        "    'I absolutely love this product!',\n",
        "    'Do not buy this, it’s horrible.'\n",
        "]\n",
        "new_sequences = tokenizer.texts_to_sequences(comments)\n",
        "new_X = pad_sequences(new_sequences, maxlen=max_length)\n",
        "\n",
        "# Predict sentiments (0 = negative, 1 = positive)\n",
        "predictions = model.predict(new_X)\n",
        "predicted_labels = [1 if p > 0.5 else 0 for p in predictions]\n",
        "\n",
        "# Print predictions\n",
        "for comment, label in zip(comments, predicted_labels):\n",
        "    sentiment = 'Positive' if label == 1 else 'Negative'\n",
        "    print(f\"Comment: '{comment}' -> Sentiment: {sentiment}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download the VADER lexicon\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Initialize the SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Sample comments without labels\n",
        "comments = [\n",
        "    'I love this product!',\n",
        "    'This is the worst purchase I have made.',\n",
        "    'Amazing quality, will buy again!',\n",
        "    'Terrible, waste of money.',\n",
        "    'Very satisfied with the service.',\n",
        "    'I regret buying this, it’s awful.',\n",
        "    'Excellent experience, highly recommend!',\n",
        "    'Do not buy this, it’s horrible.'\n",
        "]\n",
        "\n",
        "# Analyze sentiment for each comment\n",
        "for comment in comments:\n",
        "    sentiment_score = sia.polarity_scores(comment)\n",
        "    print(f\"Comment: '{comment}' -> Sentiment: {sentiment_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7TDAMa657Dc",
        "outputId": "734564b7-242b-4f44-e01a-d4edae8de839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comment: 'I love this product!' -> Sentiment: {'neg': 0.0, 'neu': 0.308, 'pos': 0.692, 'compound': 0.6696}\n",
            "Comment: 'This is the worst purchase I have made.' -> Sentiment: {'neg': 0.406, 'neu': 0.594, 'pos': 0.0, 'compound': -0.6249}\n",
            "Comment: 'Amazing quality, will buy again!' -> Sentiment: {'neg': 0.0, 'neu': 0.494, 'pos': 0.506, 'compound': 0.6239}\n",
            "Comment: 'Terrible, waste of money.' -> Sentiment: {'neg': 0.747, 'neu': 0.253, 'pos': 0.0, 'compound': -0.7096}\n",
            "Comment: 'Very satisfied with the service.' -> Sentiment: {'neg': 0.0, 'neu': 0.564, 'pos': 0.436, 'compound': 0.4754}\n",
            "Comment: 'I regret buying this, it’s awful.' -> Sentiment: {'neg': 0.659, 'neu': 0.341, 'pos': 0.0, 'compound': -0.7003}\n",
            "Comment: 'Excellent experience, highly recommend!' -> Sentiment: {'neg': 0.0, 'neu': 0.228, 'pos': 0.772, 'compound': 0.7773}\n",
            "Comment: 'Do not buy this, it’s horrible.' -> Sentiment: {'neg': 0.412, 'neu': 0.588, 'pos': 0.0, 'compound': -0.5423}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    }
  ]
}